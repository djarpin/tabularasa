from sklearn.base import RegressorMixin
from skorch import NeuralNet
from MixedMonotonicNet import MixedMonotonicNet


# TODO: can't I just specify a fit_params dict and make this whole thing way easier?


class SimultaneousQuantileMixedMonotonicNet(MixedMonotonicNet):

    def __init__(self,
                 non_monotonic_net,
                 dim_non_monotonic,
                 dim_monotonic,
                 layers=[512, 512, 64],
                 dim_out=1,
                 integration_steps=50,
                 device='cpu'):
        super().__init__()
        self.non_monotonic_net = non_monotonic_net
        self.umnn = SlowDMonotonicNN(dim_monotonic,
                                     dim_non_monotonic + 1,
                                     layers,
                                     dim_out,
                                     integration_steps,
                                     device)
        self.q = None

    def set_q(self, q=None):
        if q == None:
            self.q = None
        elif isinstance(q, float):
            if q >= 0 and q <= 1:
                self.q == q
        elif isinstance(q, int):
            if q >= 0 and q <= 100:
                self.q = q / 100.
        # Else raise an error

    def forward(self, X_monotonic, X_non_monotonic):
        h = self.non_monotonic_net(X_non_monotonic)
        if self.q == None:
            qs = torch.rand(X_non_monotonic.size(0), 1)
        else:
            qs = torch.full((X_non_monotonic.size(0), 1), self.q)
        h = torch.cat([h, qs], 1)
        return self.umnn(X_monotonic, h)


class SimultaneousQuantileRegressor(NeuralNet, RegressorMixin):

    def __init__(self, module=SimultaneousQuantileMixedMonotonicNet, *args, **kwargs):
        super(SimultaneousQuantileRegressor, self).__init__(module, *args, **kwargs)

    # Have to add a dimension to the first layer in the input network
    # Maybe I should do class inheritance on MixedNet() (see above)?
    # There appears to be an example of changing it here otherwise
    # which I could do in the __init__ of this class
    # https://pytorch.org/docs/stable/generated/torch.full.html

    def get_loss(self, y_pred, y_true, X, qs=None, *args, **kwargs):
        diff = y_pred - y_true
        threshold = (diff.ge(0).float() - qs).detach()
        return (threshold * diff).mean()

    def train_step_single(self, batch, **fit_params):
        """Compute y_pred, loss value, and update net's gradients.
        The module is set to be in train mode (e.g. dropout is
        applied).
        Parameters
        ----------
        batch
          A single batch returned by the data loader.
        **fit_params : dict
          Additional parameters passed to the ``forward`` method of
          the module and to the ``self.train_split`` call.
        Returns
        -------
        step : dict
          A dictionary ``{'loss': loss, 'y_pred': y_pred}``, where the
          float ``loss`` is the result of the loss function and
          ``y_pred`` the prediction generated by the PyTorch module.
        """
        # THIS ISN'T GOING TO WORK
        # fit_params has the dicts of data
        # I think my only choice is to augment the data once at the beginning.  I'm not sure that will work well though.
        self._set_training(True)
        Xi, yi = unpack_data(batch)
        qs = torch.rand(Xi.size(0), 1)
        Xi = torch.cat([Xi, qs], 1)
        y_pred = self.infer(Xi, **fit_params)
        loss = self.get_loss(y_pred, yi, X=Xi, training=True, qs=qs)
        loss.backward()
        return {
            'loss': loss,
            'y_pred': y_pred,
        }
