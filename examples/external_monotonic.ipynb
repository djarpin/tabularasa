{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0084b9a",
   "metadata": {},
   "source": [
    "# External Monotonic\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook trains a similar network to [mixed_monotonic](./mixed_monotonic.ipynb), but with a third party library [`TabTransformer`](https://github.com/lucidrains/tab-transformer-pytorch) as the non-monotonic network (rather than a simple user-defined embedding network).\n",
    "\n",
    "Notice:\n",
    "- We're importing `MixedMonotonicNet()` from `tabularasa`.  However, we'll define a class that inherits from this to align with `TabTransformer` inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b03dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tabularasa.MixedMonotonic import MixedMonotonicRegressor, MixedMonotonicNet\n",
    "from tab_transformer_pytorch import TabTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b5bac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load example data\n",
    "\n",
    "If you haven't already, please generate the example dataset using the [example_data](example_data.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e29f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/simple_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac66511",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Define the mixed monotonic network to interoperate with `TabTransformer`\n",
    "\n",
    "Looking at the source code for `MixedMonotonicNet`, we can see we need to rewrite the `.forward()` method.  Instead of passing a single argument `X_non_monotonic`, inputs are broken up into `X_categorical` and `X_non_monotonic`.  This is to map to `TabTransformer`s arguments of `x_categ` and `x_cont`, which serves as are non-monotonic network.  Afterward, we can continue to send the outputs from `TabTransformer` as inputs to our monotonic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86fee8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMixedMonotonicNet(MixedMonotonicNet):\n",
    "\n",
    "    def forward(self, X_monotonic, X_categorical, X_non_monotonic, last_hidden_layer=False):\n",
    "        h = self.non_monotonic_net(x_categ=X_categorical, x_cont=X_non_monotonic)\n",
    "        return self.monotonic_net(X_monotonic, h, last_hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4cd3c9",
   "metadata": {},
   "source": [
    "Now, when initializing `MixedMonotonicRegressor()`, we need to pass:\n",
    "- `module`: should now be our `CustomMixedMonotonicNet`.\n",
    "- `non_monotonic_net`: should now be `TabTransformer()`, which requires several of its own inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3afde292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MixedMonotonicRegressor(CustomMixedMonotonicNet,\n",
    "                                max_epochs=150,\n",
    "                                lr=0.01,\n",
    "                                optimizer=torch.optim.Adam,\n",
    "                                iterator_train__shuffle=True,\n",
    "                                module__non_monotonic_net=TabTransformer(categories=(50,),\n",
    "                                                                         num_continuous=1,\n",
    "                                                                         dim=16,\n",
    "                                                                         dim_out=16,\n",
    "                                                                         depth=6,\n",
    "                                                                         heads=8,\n",
    "                                                                         attn_dropout=0.1,\n",
    "                                                                         ff_dropout=0.1,\n",
    "                                                                         mlp_hidden_mults=(4, 2),\n",
    "                                                                         mlp_act=nn.ReLU()),\n",
    "                                module__dim_non_monotonic=16,\n",
    "                                module__dim_monotonic=2,\n",
    "                                module__layers=[128, 128, 32],\n",
    "                                module__integration_steps=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8f1fb",
   "metadata": {},
   "source": [
    "The `.forward()` pass now has 3 keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aae484d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.4001\u001b[0m        \u001b[32m0.2633\u001b[0m  8.0191\n",
      "      2        \u001b[36m0.2378\u001b[0m        \u001b[32m0.1617\u001b[0m  7.4229\n",
      "      3        \u001b[36m0.1449\u001b[0m        \u001b[32m0.0870\u001b[0m  6.9614\n",
      "      4        \u001b[36m0.1038\u001b[0m        0.1072  7.1651\n",
      "      5        0.1122        0.1027  7.2826\n",
      "      6        \u001b[36m0.0980\u001b[0m        0.0898  7.1615\n",
      "      7        \u001b[36m0.0972\u001b[0m        \u001b[32m0.0814\u001b[0m  7.2189\n",
      "      8        \u001b[36m0.0945\u001b[0m        \u001b[32m0.0776\u001b[0m  7.1620\n",
      "      9        \u001b[36m0.0930\u001b[0m        \u001b[32m0.0762\u001b[0m  7.3575\n",
      "     10        \u001b[36m0.0896\u001b[0m        0.0763  7.2404\n",
      "     11        \u001b[36m0.0896\u001b[0m        \u001b[32m0.0761\u001b[0m  7.0147\n",
      "     12        \u001b[36m0.0887\u001b[0m        \u001b[32m0.0757\u001b[0m  7.2891\n",
      "     13        0.0893        0.0822  7.4428\n",
      "     14        0.0906        0.0789  7.8128\n",
      "     15        0.0894        0.0759  6.7117\n",
      "     16        \u001b[36m0.0887\u001b[0m        0.0759  6.8119\n",
      "     17        0.0890        \u001b[32m0.0756\u001b[0m  6.7474\n",
      "     18        0.0895        \u001b[32m0.0754\u001b[0m  6.7389\n",
      "     19        0.0897        0.0810  6.6912\n",
      "     20        0.0923        0.0878  6.7305\n",
      "     21        0.0897        0.0768  6.8101\n",
      "     22        0.0916        0.0787  6.7489\n",
      "     23        0.0891        0.0783  6.7881\n",
      "     24        \u001b[36m0.0871\u001b[0m        \u001b[32m0.0751\u001b[0m  6.7242\n",
      "     25        \u001b[36m0.0859\u001b[0m        0.0795  6.8331\n",
      "     26        \u001b[36m0.0807\u001b[0m        0.0794  6.9557\n",
      "     27        \u001b[36m0.0804\u001b[0m        0.0847  6.7158\n",
      "     28        \u001b[36m0.0766\u001b[0m        0.0762  6.7578\n",
      "     29        \u001b[36m0.0743\u001b[0m        \u001b[32m0.0675\u001b[0m  6.7357\n",
      "     30        \u001b[36m0.0695\u001b[0m        \u001b[32m0.0656\u001b[0m  6.7484\n",
      "     31        \u001b[36m0.0689\u001b[0m        0.0694  6.7019\n",
      "     32        \u001b[36m0.0682\u001b[0m        \u001b[32m0.0633\u001b[0m  6.7033\n",
      "     33        \u001b[36m0.0669\u001b[0m        \u001b[32m0.0617\u001b[0m  6.6826\n",
      "     34        \u001b[36m0.0658\u001b[0m        0.0617  6.6425\n",
      "     35        \u001b[36m0.0636\u001b[0m        0.0651  6.7143\n",
      "     36        \u001b[36m0.0623\u001b[0m        0.0625  6.6889\n",
      "     37        0.0625        \u001b[32m0.0581\u001b[0m  6.8051\n",
      "     38        0.0642        \u001b[32m0.0580\u001b[0m  6.7385\n",
      "     39        \u001b[36m0.0605\u001b[0m        \u001b[32m0.0573\u001b[0m  6.7957\n",
      "     40        0.0608        0.0600  6.7050\n",
      "     41        \u001b[36m0.0601\u001b[0m        0.0609  6.8625\n",
      "     42        0.0602        \u001b[32m0.0564\u001b[0m  6.7888\n",
      "     43        \u001b[36m0.0590\u001b[0m        \u001b[32m0.0563\u001b[0m  6.7328\n",
      "     44        \u001b[36m0.0586\u001b[0m        0.0578  6.8789\n",
      "     45        \u001b[36m0.0585\u001b[0m        \u001b[32m0.0558\u001b[0m  6.8009\n",
      "     46        \u001b[36m0.0581\u001b[0m        \u001b[32m0.0550\u001b[0m  6.7859\n",
      "     47        0.0581        0.0551  6.8457\n",
      "     48        0.0593        0.0620  6.7945\n",
      "     49        \u001b[36m0.0579\u001b[0m        \u001b[32m0.0548\u001b[0m  6.7837\n",
      "     50        \u001b[36m0.0573\u001b[0m        \u001b[32m0.0541\u001b[0m  6.8529\n",
      "     51        0.0589        0.0550  6.7853\n",
      "     52        0.0615        0.0621  6.7918\n",
      "     53        0.0586        0.0557  6.7007\n",
      "     54        0.0606        0.0581  6.7972\n",
      "     55        0.0593        0.0588  6.9777\n",
      "     56        0.0592        0.0584  6.7513\n",
      "     57        0.0585        0.0572  6.7310\n",
      "     58        0.0581        0.0549  6.7702\n",
      "     59        \u001b[36m0.0566\u001b[0m        0.0552  6.7507\n",
      "     60        0.0570        0.0630  6.7792\n",
      "     61        0.0578        0.0559  6.8708\n",
      "     62        0.0570        0.0557  6.7974\n",
      "     63        0.0572        0.0564  6.7394\n",
      "     64        0.0580        0.0636  6.7712\n",
      "     65        0.0596        0.0546  6.6911\n",
      "     66        0.0598        0.0595  6.7707\n",
      "     67        0.0606        0.0574  6.7571\n",
      "     68        0.0591        0.0632  6.6810\n",
      "     69        0.0605        0.0560  6.8107\n",
      "     70        0.0579        \u001b[32m0.0539\u001b[0m  6.7190\n",
      "     71        0.0599        0.0551  6.7320\n",
      "     72        0.0567        \u001b[32m0.0537\u001b[0m  6.8252\n",
      "     73        \u001b[36m0.0560\u001b[0m        0.0540  6.8884\n",
      "     74        0.0564        \u001b[32m0.0536\u001b[0m  6.7815\n",
      "     75        0.0564        0.0576  6.8097\n",
      "     76        0.0604        0.0538  6.7108\n",
      "     77        0.0569        0.0553  6.7845\n",
      "     78        0.0581        0.0551  6.8140\n",
      "     79        0.0589        0.0541  6.9005\n",
      "     80        0.0564        0.0539  6.7930\n",
      "     81        0.0566        0.0539  6.7785\n",
      "     82        \u001b[36m0.0560\u001b[0m        \u001b[32m0.0533\u001b[0m  6.7483\n",
      "     83        0.0566        0.0535  6.8078\n",
      "     84        0.0593        0.0572  6.7865\n",
      "     85        0.0575        0.0539  6.9603\n",
      "     86        0.0573        0.0572  6.7630\n",
      "     87        0.0568        0.0558  6.7479\n",
      "     88        0.0568        0.0542  6.7417\n",
      "     89        \u001b[36m0.0560\u001b[0m        0.0552  7.8005\n",
      "     90        0.0563        0.0535  6.8796\n",
      "     91        0.0564        0.0539  6.8243\n",
      "     92        0.0572        0.0541  6.7829\n",
      "     93        0.0589        0.0577  6.7168\n",
      "     94        0.0608        0.0659  6.7370\n",
      "     95        0.0608        0.0670  6.6667\n",
      "     96        0.0649        0.0549  6.7973\n",
      "     97        0.0598        \u001b[32m0.0533\u001b[0m  6.8841\n",
      "     98        \u001b[36m0.0557\u001b[0m        0.0533  6.7360\n",
      "     99        0.0557        0.0541  6.8347\n",
      "    100        0.0560        0.0537  7.0099\n",
      "    101        0.0558        \u001b[32m0.0531\u001b[0m  6.7687\n",
      "    102        0.0558        0.0532  6.7919\n",
      "    103        0.0564        0.0532  6.8572\n",
      "    104        0.0568        0.0536  6.7392\n",
      "    105        0.0582        0.0587  6.7794\n",
      "    106        0.0576        0.0597  6.7718\n",
      "    107        0.0594        0.0608  6.7146\n",
      "    108        0.0612        0.0597  6.8172\n",
      "    109        0.0599        0.0553  6.7227\n",
      "    110        0.0570        0.0533  6.7916\n",
      "    111        0.0564        0.0534  6.8151\n",
      "    112        0.0562        \u001b[32m0.0529\u001b[0m  6.7916\n",
      "    113        0.0568        0.0531  6.7083\n",
      "    114        0.0571        0.0599  6.8718\n",
      "    115        0.0585        0.0532  6.9117\n",
      "    116        0.0566        0.0561  6.8719\n",
      "    117        0.0577        0.0562  6.7559\n",
      "    118        0.0576        0.0553  6.7309\n",
      "    119        0.0571        0.0550  6.8275\n",
      "    120        0.0573        0.0530  6.8812\n",
      "    121        0.0572        0.0530  6.7100\n",
      "    122        0.0567        0.0535  6.7540\n",
      "    123        0.0562        0.0532  6.8051\n",
      "    124        0.0564        0.0536  6.8169\n",
      "    125        0.0585        0.0534  6.9090\n",
      "    126        0.0622        0.0535  6.7895\n",
      "    127        0.0596        0.0552  6.7447\n",
      "    128        0.0588        0.0667  6.7626\n",
      "    129        0.0605        0.0562  7.1684\n",
      "    130        0.0584        0.0541  7.3083\n",
      "    131        0.0558        0.0532  7.1487\n",
      "    132        0.0575        0.0544  7.1512\n",
      "    133        0.0573        0.0540  7.1382\n",
      "    134        0.0559        \u001b[32m0.0529\u001b[0m  7.2239\n",
      "    135        0.0558        0.0540  7.0436\n",
      "    136        0.0561        \u001b[32m0.0528\u001b[0m  6.8653\n",
      "    137        0.0557        0.0550  6.9111\n",
      "    138        0.0565        0.0539  6.9331\n",
      "    139        0.0559        0.0535  6.9007\n",
      "    140        0.0564        0.0532  6.8046\n",
      "    141        0.0565        0.0533  6.7199\n",
      "    142        0.0575        0.0552  6.7295\n",
      "    143        0.0578        0.0546  6.7711\n",
      "    144        0.0563        0.0532  6.7394\n",
      "    145        0.0561        0.0531  6.7227\n",
      "    146        0.0565        0.0532  6.7519\n",
      "    147        0.0559        0.0558  6.7728\n",
      "    148        0.0559        0.0530  6.8075\n",
      "    149        \u001b[36m0.0556\u001b[0m        0.0530  6.9588\n",
      "    150        0.0557        0.0534  6.8806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'tabularasa.MixedMonotonic.MixedMonotonicRegressor'>[initialized](\n",
       "  module_=CustomMixedMonotonicNet(\n",
       "    (non_monotonic_net): TabTransformer(\n",
       "      (norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (embeds): Embedding(52, 16)\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=384, bias=False)\n",
       "                  (to_out): Linear(in_features=128, out_features=16, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                    (1): GEGLU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=384, bias=False)\n",
       "                  (to_out): Linear(in_features=128, out_features=16, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                    (1): GEGLU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=384, bias=False)\n",
       "                  (to_out): Linear(in_features=128, out_features=16, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                    (1): GEGLU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=384, bias=False)\n",
       "                  (to_out): Linear(in_features=128, out_features=16, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                    (1): GEGLU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=384, bias=False)\n",
       "                  (to_out): Linear(in_features=128, out_features=16, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                    (1): GEGLU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=384, bias=False)\n",
       "                  (to_out): Linear(in_features=128, out_features=16, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                    (1): GEGLU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=17, out_features=8, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=4, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (umnn): SlowDMonotonicNN(\n",
       "      (outer_net): MonotonicNN(\n",
       "        (integrand): IntegrandNN(\n",
       "          (inner_net): Sequential(\n",
       "            (0): Linear(in_features=17, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=17, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=32, out_features=1, bias=True)\n",
       "            (7): ELU(alpha=1.0)\n",
       "          )\n",
       "        )\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (5): ReLU()\n",
       "          (6): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit({'X_monotonic': df[['x1', 'x2']].values,\n",
    "           'X_categorical': df[['x3']].values,\n",
    "           'X_non_monotonic': df[['x4']].values},\n",
    "          df[['y']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dac7d0",
   "metadata": {},
   "source": [
    "We'll create a partial dependence plot to visualize how the monotonic network constrained our relationship between `x1` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fad5760",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48020146",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for q in np.quantile(df['x1'], quantiles):\n",
    "    dfc = df.copy()\n",
    "    dfc['x1'] = q\n",
    "    dfc['x1'] = dfc['x1'].astype('float32')\n",
    "    p = model.predict({'X_monotonic': dfc[['x1', 'x2']].values,\n",
    "                       'X_categorical': dfc[['x3']].values,\n",
    "                       'X_non_monotonic': dfc[['x4']].values})\n",
    "    p = pd.DataFrame(pd.Series(p[:, 0]).describe(percentiles=quantiles)).T\n",
    "    p['x1'] = q\n",
    "    results.append(p)\n",
    "results = pd.concat(results, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae7577",
   "metadata": {},
   "source": [
    "We can see below that, unlike with the simple MLP network, we have a monotonically increasing relationship between `x1` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5b445dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.plot(x='x1', y=['20%', 'mean', '80%'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5671e8",
   "metadata": {},
   "source": [
    "The next [example](./tabula_rasa.ipynb) walks through `TabulaRasaRegressor()`, which combines logic from all prior examples into a single, easy-to-use class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
