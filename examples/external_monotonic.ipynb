{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0084b9a",
   "metadata": {},
   "source": [
    "# External Monotonic\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook trains a similar network to [mixed_monotonic](./mixed_monotonic.ipynb), but with a third party library [`TabTransformer`](https://github.com/lucidrains/tab-transformer-pytorch) as the non-monotonic network (rather than a simple user-defined embedding network).\n",
    "\n",
    "Notice:\n",
    "- We're importing `MixedMonotonicNet()` from `tabularasa`.  However, we'll define a class that inherits from this to align with `TabTransformer` inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b03dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tabularasa.MixedMonotonic import MixedMonotonicRegressor, MixedMonotonicNet\n",
    "from tab_transformer_pytorch import TabTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b5bac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load example data\n",
    "\n",
    "If you haven't already, please generate the example dataset using the [example_data](example_data.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e29f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/simple_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac66511",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Define the mixed monotonic network to interoperate with `TabTransformer`\n",
    "\n",
    "Looking at the source code for `MixedMonotonicNet`, we can see we need to rewrite the `.forward()` method.  Instead of passing a single argument `X_non_monotonic`, inputs are broken up into `X_categorical` and `X_non_monotonic`.  This is to map to `TabTransformer`s arguments of `x_categ` and `x_cont`, which serves as are non-monotonic network.  Afterward, we can continue to send the outputs from `TabTransformer` as inputs to our monotonic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fee8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMixedMonotonicNet(MixedMonotonicNet):\n",
    "\n",
    "    def forward(self, X_monotonic, X_categorical, X_non_monotonic, last_hidden_layer=False):\n",
    "        h = self.non_monotonic_net(x_categ=X_categorical, x_cont=X_non_monotonic)\n",
    "        return self.monotonic_net(X_monotonic, h, last_hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4cd3c9",
   "metadata": {},
   "source": [
    "Now, when initializing `MixedMonotonicRegressor()`, we need to pass:\n",
    "- `module`: should now be our `CustomMixedMonotonicNet`.\n",
    "- `non_monotonic_net`: should now be `TabTransformer()`, which requires several of its own inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3afde292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MixedMonotonicRegressor(CustomMixedMonotonicNet,\n",
    "                                max_epochs=150,\n",
    "                                lr=0.01,\n",
    "                                optimizer=torch.optim.Adam,\n",
    "                                iterator_train__shuffle=True,\n",
    "                                module__non_monotonic_net=TabTransformer(categories=(50,),\n",
    "                                                                         num_continuous=1,\n",
    "                                                                         dim=16,\n",
    "                                                                         dim_out=16,\n",
    "                                                                         depth=6,\n",
    "                                                                         heads=8,\n",
    "                                                                         attn_dropout=0.1,\n",
    "                                                                         ff_dropout=0.1,\n",
    "                                                                         mlp_hidden_mults=(4, 2),\n",
    "                                                                         mlp_act=nn.ReLU()),\n",
    "                                module__dim_non_monotonic=16,\n",
    "                                module__dim_monotonic=2,\n",
    "                                module__layers=[128, 128, 32],\n",
    "                                module__integration_steps=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8f1fb",
   "metadata": {},
   "source": [
    "The `.forward()` pass now has 3 keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae484d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m0.4557\u001b[0m        \u001b[32m0.3768\u001b[0m  13.5676\n",
      "      2        \u001b[36m0.3030\u001b[0m        \u001b[32m0.3374\u001b[0m  13.3396\n",
      "      3        \u001b[36m0.2807\u001b[0m        \u001b[32m0.3285\u001b[0m  13.4251\n",
      "      4        \u001b[36m0.2764\u001b[0m        \u001b[32m0.3131\u001b[0m  13.0348\n",
      "      5        \u001b[36m0.2620\u001b[0m        \u001b[32m0.2996\u001b[0m  13.1527\n",
      "      6        \u001b[36m0.2510\u001b[0m        \u001b[32m0.2747\u001b[0m  13.1471\n",
      "      7        \u001b[36m0.2144\u001b[0m        \u001b[32m0.1951\u001b[0m  13.1088\n",
      "      8        \u001b[36m0.1404\u001b[0m        \u001b[32m0.0974\u001b[0m  13.1483\n",
      "      9        0.2203        0.1829  12.7633\n",
      "     10        \u001b[36m0.1266\u001b[0m        0.1495  13.2515\n",
      "     11        \u001b[36m0.1157\u001b[0m        0.1280  13.3587\n",
      "     12        \u001b[36m0.1028\u001b[0m        0.1021  13.0842\n",
      "     13        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0771\u001b[0m  13.4353\n",
      "     14        \u001b[36m0.0733\u001b[0m        \u001b[32m0.0705\u001b[0m  13.3862\n",
      "     15        \u001b[36m0.0716\u001b[0m        0.0705  13.1515\n",
      "     16        \u001b[36m0.0705\u001b[0m        0.0744  13.2425\n",
      "     17        0.0718        0.0719  13.0495\n",
      "     18        0.0730        0.0718  12.6927\n",
      "     19        0.0706        0.0768  13.3245\n",
      "     20        0.0710        0.0733  13.5802\n",
      "     21        0.0705        0.0708  13.2136\n",
      "     22        0.0706        0.0705  13.3561\n",
      "     23        0.0709        \u001b[32m0.0704\u001b[0m  13.2624\n",
      "     24        0.0716        0.0752  13.4097\n",
      "     25        0.0776        0.0926  13.0934\n",
      "     26        0.0759        0.0781  13.4655\n",
      "     27        0.0738        0.0723  12.7122\n",
      "     28        0.0731        0.0719  13.3976\n",
      "     29        0.0722        0.0707  13.3649\n",
      "     30        0.0713        \u001b[32m0.0703\u001b[0m  13.2835\n",
      "     31        0.0712        0.0724  12.7669\n",
      "     32        0.0730        0.0807  13.3977\n",
      "     33        0.0745        0.0786  13.2475\n",
      "     34        0.0715        0.0724  13.3910\n",
      "     35        0.0711        0.0720  13.5068\n",
      "     36        0.0721        \u001b[32m0.0702\u001b[0m  13.2142\n",
      "     37        0.0717        0.0710  13.3128\n",
      "     38        0.0716        0.0703  13.0408\n",
      "     39        0.0718        0.0733  13.1617\n",
      "     40        0.0732        0.0805  13.2778\n",
      "     41        0.0730        0.0790  13.3023\n",
      "     42        0.0724        0.0734  13.1319\n",
      "     43        0.0709        0.0729  13.2090\n",
      "     44        0.0706        0.0723  13.1342\n",
      "     45        0.0711        0.0719  12.5514\n",
      "     46        0.0709        0.0703  13.4437\n",
      "     47        0.0714        0.0707  14.0263\n",
      "     48        0.0708        \u001b[32m0.0702\u001b[0m  14.0614\n",
      "     49        0.0707        0.0706  14.8113\n",
      "     50        0.0706        0.0710  14.1013\n",
      "     51        0.0718        0.0706  14.3782\n",
      "     52        0.0710        0.0702  14.0526\n",
      "     53        0.0706        \u001b[32m0.0700\u001b[0m  13.8020\n",
      "     54        0.0709        \u001b[32m0.0697\u001b[0m  14.3367\n",
      "     55        0.0717        0.0716  14.2520\n",
      "     56        0.0714        0.0711  14.2008\n",
      "     57        0.0706        0.0707  14.2416\n",
      "     58        0.0714        0.0764  13.9888\n",
      "     59        0.0723        0.0729  14.3332\n",
      "     60        0.0711        0.0706  14.3138\n",
      "     61        0.0721        0.0713  14.0144\n",
      "     62        0.0733        0.0715  14.3967\n",
      "     63        0.0718        0.0714  14.6078\n",
      "     64        0.0721        0.0705  14.6123\n",
      "     65        0.0712        0.0759  14.4214\n",
      "     66        0.0722        0.0817  14.3621\n",
      "     67        0.0719        0.0731  14.2570\n",
      "     68        0.0707        0.0725  14.1673\n",
      "     69        \u001b[36m0.0704\u001b[0m        \u001b[32m0.0697\u001b[0m  14.5990\n",
      "     70        0.0724        0.0713  14.3815\n",
      "     71        0.0718        0.0714  14.6122\n",
      "     72        0.0713        0.0724  14.2257\n",
      "     73        0.0706        0.0720  13.9191\n",
      "     74        0.0719        0.0723  14.4694\n",
      "     75        0.0705        0.0708  14.5137\n",
      "     76        0.0704        0.0727  14.0141\n",
      "     77        \u001b[36m0.0703\u001b[0m        0.0709  14.7357\n",
      "     78        0.0707        0.0724  14.7636\n",
      "     79        0.0705        0.0753  14.4744\n",
      "     80        0.0720        0.0773  14.1569\n",
      "     81        0.0729        0.0736  14.6336\n",
      "     82        0.0712        0.0710  14.4496\n",
      "     83        0.0709        0.0707  14.0291\n",
      "     84        0.0712        0.0707  14.3266\n",
      "     85        0.0720        0.0701  14.0491\n",
      "     86        0.0704        0.0715  14.2381\n",
      "     87        0.0705        0.0711  14.4908\n",
      "     88        \u001b[36m0.0703\u001b[0m        0.0701  14.4445\n",
      "     89        0.0709        0.0716  14.7328\n",
      "     90        0.0708        0.0704  14.2229\n",
      "     91        0.0708        0.0713  14.3556\n",
      "     92        0.0716        0.0721  14.7077\n",
      "     93        \u001b[36m0.0703\u001b[0m        0.0699  14.6061\n",
      "     94        0.0707        0.0710  14.2910\n",
      "     95        0.0712        0.0706  13.9109\n",
      "     96        0.0718        0.0699  14.0946\n",
      "     97        0.0705        0.0700  13.7900\n",
      "     98        0.0708        0.0703  14.2994\n",
      "     99        0.0709        0.0703  14.7377\n",
      "    100        0.0726        0.0790  14.2061\n",
      "    101        0.0735        0.0774  14.4636\n",
      "    102        0.0729        0.0744  14.4226\n",
      "    103        0.0710        0.0778  14.4599\n",
      "    104        0.0708        0.0708  13.7659\n",
      "    105        0.0704        0.0701  14.6825\n",
      "    106        0.0705        0.0707  14.0905\n",
      "    107        \u001b[36m0.0703\u001b[0m        0.0703  14.3920\n",
      "    108        0.0709        0.0704  14.6437\n",
      "    109        0.0715        0.0716  14.4078\n",
      "    110        0.0725        0.0704  14.2114\n",
      "    111        0.0712        0.0704  14.7768\n",
      "    112        0.0707        0.0720  14.2448\n",
      "    113        0.0704        0.0722  14.1235\n",
      "    114        0.0710        0.0731  13.7941\n",
      "    115        0.0705        0.0701  13.8055\n",
      "    116        0.0706        0.0700  13.9853\n",
      "    117        0.0705        0.0703  13.7154\n",
      "    118        0.0706        0.0713  14.4600\n",
      "    119        0.0712        \u001b[32m0.0696\u001b[0m  13.7808\n",
      "    120        0.0706        0.0726  14.3928\n",
      "    121        0.0707        0.0738  14.6099\n",
      "    122        0.0718        0.0700  14.3277\n",
      "    123        0.0705        0.0708  14.7589\n",
      "    124        0.0706        0.0704  14.7006\n",
      "    125        0.0713        0.0714  14.0939\n",
      "    126        0.0705        0.0734  14.1947\n",
      "    127        0.0711        0.0705  14.2712\n",
      "    128        0.0703        0.0712  14.3142\n",
      "    129        0.0704        0.0728  13.6412\n",
      "    130        0.0708        0.0758  14.8098\n",
      "    131        0.0728        0.0766  14.6036\n",
      "    132        \u001b[36m0.0701\u001b[0m        0.0701  14.2520\n",
      "    133        0.0707        0.0750  14.5157\n",
      "    134        0.0713        0.0717  14.4606\n",
      "    135        0.0706        0.0735  14.4573\n",
      "    136        0.0714        0.0723  14.5563\n",
      "    137        0.0707        0.0718  14.5177\n",
      "    138        0.0705        0.0703  14.8858\n",
      "    139        0.0703        0.0714  14.6185\n",
      "    140        \u001b[36m0.0701\u001b[0m        0.0698  13.8628\n",
      "    141        0.0710        0.0702  14.1771\n",
      "    142        0.0711        0.0699  14.4393\n",
      "    143        0.0730        0.0751  14.2044\n",
      "    144        0.0748        0.0779  14.3404\n",
      "    145        0.0735        0.0855  14.3895\n",
      "    146        0.0734        0.0817  14.6223\n",
      "    147        0.0760        0.0757  14.6164\n",
      "    148        0.0725        0.0761  14.5397\n",
      "    149        0.0724        0.0744  14.7784\n",
      "    150        0.0720        0.0708  14.2839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'tabularasa.MixedMonotonic.MixedMonotonicRegressor'>[initialized](\n",
       "  module_=CustomMixedMonotonicNet(\n",
       "    (non_monotonic_net): TabTransformer(\n",
       "      (norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (embeds): Embedding(52, 16)\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=384, bias=False)\n",
       "                  (to_out): Linear(in_features=128, out_features=16, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                    (1): GEGLU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=384, bias=False)\n",
       "                  (to_out): Linear(in_features=128, out_features=16, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                    (1): GEGLU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=384, bias=False)\n",
       "                  (to_out): Linear(in_features=128, out_features=16, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                    (1): GEGLU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=384, bias=False)\n",
       "                  (to_out): Linear(in_features=128, out_features=16, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                    (1): GEGLU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=384, bias=False)\n",
       "                  (to_out): Linear(in_features=128, out_features=16, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                    (1): GEGLU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): ModuleList(\n",
       "            (0): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=384, bias=False)\n",
       "                  (to_out): Linear(in_features=128, out_features=16, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "                    (1): GEGLU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=17, out_features=8, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=4, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (monotonic_net): SlowDMonotonicNN(\n",
       "      (outer_net): MonotonicNN(\n",
       "        (integrand): IntegrandNN(\n",
       "          (inner_net): Sequential(\n",
       "            (0): Linear(in_features=17, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (5): ReLU()\n",
       "          )\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=17, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (5): ReLU()\n",
       "            (6): Linear(in_features=32, out_features=1, bias=True)\n",
       "            (7): ELU(alpha=1.0)\n",
       "          )\n",
       "        )\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (5): ReLU()\n",
       "          (6): Linear(in_features=32, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit({'X_monotonic': df[['x1', 'x2']].values,\n",
    "           'X_categorical': df[['x3']].values,\n",
    "           'X_non_monotonic': df[['x4']].values},\n",
    "          df[['y']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dac7d0",
   "metadata": {},
   "source": [
    "We'll create a partial dependence plot to visualize how the monotonic network constrained our relationship between `x1` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fad5760",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48020146",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for q in np.quantile(df['x1'], quantiles):\n",
    "    dfc = df.copy()\n",
    "    dfc['x1'] = q\n",
    "    dfc['x1'] = dfc['x1'].astype('float32')\n",
    "    p = model.predict({'X_monotonic': dfc[['x1', 'x2']].values,\n",
    "                       'X_categorical': dfc[['x3']].values,\n",
    "                       'X_non_monotonic': dfc[['x4']].values})\n",
    "    p = pd.DataFrame(pd.Series(p[:, 0]).describe(percentiles=quantiles)).T\n",
    "    p['x1'] = q\n",
    "    results.append(p)\n",
    "results = pd.concat(results, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae7577",
   "metadata": {},
   "source": [
    "We can see below that, unlike with the simple MLP network, we have a monotonically increasing relationship between `x1` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5b445dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.plot(x='x1', y=['20%', 'mean', '80%'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5671e8",
   "metadata": {},
   "source": [
    "The next [example](./tabula_rasa.ipynb) walks through `TabulaRasaRegressor()`, which combines logic from all prior examples into a single, easy-to-use class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
